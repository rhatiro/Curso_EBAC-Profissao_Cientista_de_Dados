{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01fe748-314a-4fa9-bbc8-b58cd625898d",
   "metadata": {},
   "source": [
    "[![ebac_logo-data_science.png](https://raw.githubusercontent.com/rhatiro/Curso_EBAC-Profissao_Cientista_de_Dados/main/ebac-course-utils/media/logo/ebac_logo-data_science.png)](https://github.com/rhatiro/Curso_EBAC-Profissao_Cientista_de_Dados)\n",
    "<!-- <img src=\"https://raw.githubusercontent.com/rhatiro/Curso_EBAC-Profissao_Cientista_de_Dados/main/ebac-course-utils/media/logo/ebac_logo-data_science.png\" alt=\"ebac_logo-data_science\"> -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- # **Profissão: Cientista de Dados** -->\n",
    "### **Módulo 24** | Combinação de modelos II | Exercício 2\n",
    "\n",
    "**Aluno:** [Roberto Hatiro Nishiyama](https://www.linkedin.com/in/rhatiro/)<br>\n",
    "**Data:** 6 de junho de 2023.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87421f-2533-4f0f-9df1-61d76430e0bc",
   "metadata": {},
   "source": [
    "# Tarefa 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8c515-70bd-4e3d-a35e-267fc6101275",
   "metadata": {},
   "source": [
    "**1.** Cite 5 diferenças entre o AdaBoost e o GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b6575-ec87-489c-81f1-b49a29cbba70",
   "metadata": {},
   "source": [
    "| Diferenças | AdaBoost | GBM |\n",
    "| ---------- | -------- | --- |\n",
    "| 1 | Utiliza um conjunto de classificadores fracos (*Stumps*), também conhecidos como *weak/base learners*. | Utiliza árvores mais complexas do que o *AdaBoost*. |\n",
    "| 2 | As árvores de decisão são extremamente simples com apenas 1 de profundidade e 2 folhas. | As árvores complexas podem ser podadas ou não. |\n",
    "| 3 | O primeiro passo do modelo é a seleção de um *Stump* com o melhor desempenho. | O primeiro passo do modelo é o cálculo da média do Y (resposta/*target*). |\n",
    "| 4 | Cada resposta tem um peso diferente para a agregação final da predição. | A predição das respostas das árvores é calculada através de um multiplicador em comum chamado *learning_rate* (eta). |\n",
    "| 5 | A predição final é definida com uma votação majoritária ponderada das respostas de acordo com a performance de cada *Stump*. | A predição é baseada no ajuste do modelo através da redução dos erros residuais. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6d6ef-fc2c-4a3b-af12-84f47a045206",
   "metadata": {},
   "source": [
    "**2.** Acesse o link [Scikit-learn – GBM](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ae2ef-1da6-4003-88b2-07493906dc17",
   "metadata": {},
   "source": [
    "> [1.11. Ensemble methods — scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/ensemble.html#)\n",
    ">> [1.11.4.1. Classification](https://scikit-learn.org/stable/modules/ensemble.html#classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c18906a-757b-4c3b-8c96-d810bcafbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41a352d-e340-44bd-b6bf-431f9ac42aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, \n",
    "                                 learning_rate=1.0, \n",
    "                                 max_depth=1, \n",
    "                                 random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b0c1c-4407-4b0c-b4c9-e6fb8efd7bf2",
   "metadata": {},
   "source": [
    "> [1.11. Ensemble methods — scikit-learn 1.2.2 documentation](https://scikit-learn.org/stable/modules/ensemble.html#)\n",
    ">> [1.11.4.2. Regression](https://scikit-learn.org/stable/modules/ensemble.html#regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39199baf-62e1-493b-a692-0fb21529054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a068a940-b305-4c6a-bccb-cef99e59d0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960319"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "est = GradientBoostingRegressor(n_estimators=100, \n",
    "                                learning_rate=0.1, \n",
    "                                max_depth=1, \n",
    "                                random_state=0, \n",
    "                                loss='squared_error').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875633aa-d96e-460b-8487-a6743ef61926",
   "metadata": {},
   "source": [
    "**3.** Cite 5 hiperparâmetros importantes no GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86622a0-b73c-4c06-84dd-76f20ddf2fe9",
   "metadata": {},
   "source": [
    "> **1. `n_estimators`**: Este importante parâmetro controla o número de árvores no modelo. Um valor maior pode aumentar tanto o tempo de treinamento quanto a capacidade de aprendizado do modelo.\n",
    ">\n",
    "> **2. `learning_rate`**: É um importante hiperparâmetro no intervalo de 0.0 a 1.0 que controla a taxa de aprendizado do modelo. Interage fortemente com o número de estimadores (*n_estimators*). O valor ideal pode variar dependendo do problema e dos dados.\n",
    ">\n",
    "> **3. `max_depth`**: Controla a profundidade máxima de cada árvore no modelo, especificando o número máximo de nós. Uma profundidade maior permite que as árvores sejam mais complexas e tenham maior capacidade de aprendizado, mas também aumenta o risco de *overfitting*.\n",
    ">\n",
    "> **4. `max_features`**: Controla o número de variáveis consideradas ao fazer uma divisão em cada nó das árvores. Quanto menor o valor, menor é o risco de *overfitting*.\n",
    ">\n",
    "> **5. `warm_start`**: Permite adicionar mais estimadores a um modelo já ajustado. Pode ser útil quando é necessário ajustar o modelo com mais árvores sem perder o progresso anterior.\n",
    ">> Exemplo:\n",
    ">>```python\n",
    ">>>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n",
    ">>>>> _ = est.fit(X_train, y_train)  # fit additional 100 trees to est\n",
    ">>>>> mean_squared_error(y_test, est.predict(X_test))\n",
    ">>3.84...\n",
    ">>```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d6267-19e7-47d2-b795-a68bcd361515",
   "metadata": {},
   "source": [
    "**4.** (**Opcional**) Utilize o GridSearch para encontrar os melhores hiperparâmetros para o conjunto de dados do exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ac016b-f9cb-4e7c-a2b4-12b164f21e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d473856d-bb0c-495e-a17d-2632eb805035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 783 ms, total: 1min 25s\n",
      "Wall time: 1min 29s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>3.625209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "      <td>3.667692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.733205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.733272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.774987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>21.909960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9</td>\n",
       "      <td>22.400111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "      <td>22.472923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>22.902975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>24.252178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  learning_rate  max_depth  mean_squared_error\n",
       "0           1000           0.03          1            3.625209\n",
       "1           1000           0.06          1            3.667692\n",
       "2          10000           0.10          3            3.733205\n",
       "3           1000           0.10          3            3.733272\n",
       "4            100           0.10          3            3.774987\n",
       "..           ...            ...        ...                 ...\n",
       "59            10           0.03          1           21.909960\n",
       "60            10           0.01          9           22.400111\n",
       "61            10           0.01          6           22.472923\n",
       "62            10           0.01          3           22.902975\n",
       "63            10           0.01          1           24.252178\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimators     = [10, 100, 1000, 10000]\n",
    "learning_rates = [0.01, 0.03, 0.06, 0.1]\n",
    "max_depths     = [1, 3, 6, 9]\n",
    "\n",
    "grid_search = []\n",
    "\n",
    "for n in estimators:\n",
    "    for rate in learning_rates:\n",
    "        for depth in max_depths:\n",
    "            est = GradientBoostingRegressor(n_estimators=n, \n",
    "                                            learning_rate=rate, \n",
    "                                            max_depth=depth, \n",
    "                                            random_state=0, \n",
    "                                            loss='squared_error').fit(X_train, y_train)\n",
    "            grid_search.append([n, rate, depth, mean_squared_error(y_test, est.predict(X_test))])\n",
    "            \n",
    "(pd.DataFrame(data=grid_search, \n",
    "              columns=['n_estimators', 'learning_rate', 'max_depth', 'mean_squared_error'])\n",
    "   .sort_values(by='mean_squared_error', \n",
    "                ascending=True, \n",
    "                ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd77068-5da9-48bf-b7b7-c88ff2978b5e",
   "metadata": {},
   "source": [
    "**5.** Acessando o artigo do Jerome Friedman ([Stochastic](https://jerryfriedman.su.domains/ftp/stobst.pdf)) e pensando no nome dado ao **Stochastic GBM**, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3fde36-2bb7-46f9-ba26-55627bea0d8a",
   "metadata": {},
   "source": [
    "> Pensando no nome dado ao *Stochastic GBM* e com referência à teoria probabilística, podemos descrevê-lo como um algoritmo que incorpora variáveis aleatórias. O *Stochastic GBM* é uma combinação dos métodos de *Gradient Boosting* e *Bootstrap Aggregating*, sendo considerado um híbrido das técnicas *Bagging* e *Boosting*. Em cada iteração, o classificador base é treinado em um subconjunto aleatório e não repetitivo dos dados de treinamento, utilizando em média metade da amostra. Essa aleatoriedade na amostragem do conjunto de dados em cada iteração de treino melhora significativamente a precisão do *Gradient Boosting* e torna o modelo mais robusto em comparação com o GBM tradicional. Isso ocorre porque a aleatoriedade ajuda a evitar o *overfitting* e promove uma melhor generalização do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4448a-5921-4d47-84af-b29a7f008175",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
